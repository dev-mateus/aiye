# DESENVOLVIMENTO

## Estrutura do Projeto

```
umbanda-qa/
‚îú‚îÄ‚îÄ backend/                          # FastAPI + RAG
‚îÇ   ‚îú‚îÄ‚îÄ main.py                      # App FastAPI (endpoints /healthz, /ask)
‚îÇ   ‚îú‚îÄ‚îÄ rag.py                       # L√≥gica RAG (embeddings, busca, resposta)
‚îÇ   ‚îú‚îÄ‚îÄ models.py                    # Modelos Pydantic (AskRequest, AskResponse)
‚îÇ   ‚îú‚îÄ‚îÄ settings.py                  # Configura√ß√µes (carrega .env)
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py                    # Script CLI para ingerir PDFs
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt              # Depend√™ncias Python
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                  # Init do package
‚îÇ   ‚îî‚îÄ‚îÄ data/
‚îÇ       ‚îú‚îÄ‚îÄ pdfs/                    # PDFs para ingest√£o
‚îÇ       ‚îî‚îÄ‚îÄ index/                   # √çndices FAISS + metadata.json
‚îÇ
‚îú‚îÄ‚îÄ frontend/                        # React + Vite + TypeScript
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.tsx                # Entrada React
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.tsx                 # Componente raiz
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api.ts                  # Client HTTP
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ styles.css              # Tailwind + custom
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ components/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ChatBox.tsx         # Input + bot√£o
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ AnswerCard.tsx      # Exibe resposta
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ SourceList.tsx      # Lista de fontes
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îú‚îÄ‚îÄ vite.config.ts
‚îÇ   ‚îú‚îÄ‚îÄ tsconfig.json
‚îÇ   ‚îú‚îÄ‚îÄ tailwind.config.js
‚îÇ   ‚îú‚îÄ‚îÄ postcss.config.js
‚îÇ   ‚îî‚îÄ‚îÄ index.html
‚îÇ
‚îú‚îÄ‚îÄ .env.example                     # Vari√°veis de exemplo
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md                        # Documenta√ß√£o principal
‚îú‚îÄ‚îÄ QUICKSTART.md                    # Guia de in√≠cio r√°pido
‚îî‚îÄ‚îÄ DEVELOPMENT.md                   # Este arquivo
```

## Fluxo de RAG

### 1. Ingest√£o (backend/ingest.py)

```python
python backend/ingest.py
```

- L√™ PDFs de `backend/data/pdfs/`
- Extrai texto com PyMuPDF
- Divide em chunks (~1200 chars, overlap ~150 chars)
- Gera embeddings com SentenceTransformer
- Salva √≠ndice FAISS: `backend/data/index/index.faiss`
- Salva metadados: `backend/data/index/metadata.json`

### 2. Query (backend/main.py POST /ask)

```python
POST /ask
{
  "question": "O que √© Umbanda?"
}
```

- Recebe pergunta do usu√°rio
- Valida (m√≠nimo 3 caracteres)
- Embeda pergunta
- Busca top-5 chunks mais similares no FAISS
- Filtra por min_similarity (0.25)
- Expans√£o de query: por padr√£o usa apenas sin√¥nimos controlados (LLM desativado)
- Chama `generate_answer()` com contextos
- Retorna resposta + fontes + metadados

### 3. Resposta (backend/rag.py generate_answer)

```python
def generate_answer(question: str, contexts: list[dict]) -> str:
    # Atualmente: concatena contextos + aviso √©tico
    # TODO: Integrar com LLM externo (Copilot/M365/OpenAI)
```

Placeholder que:
- Agrupa contextos por documento
- Monta resposta leg√≠vel
- Adiciona aviso √©tico
- Retorna string formatada

## Modifica√ß√µes Comuns

### Adicionar novo endpoint

Em `backend/main.py`:

```python
@app.post("/novo-endpoint")
async def novo_endpoint(request: SomeRequest) -> SomeResponse:
    # Implementar l√≥gica
    return response
```

### Integrar com LLM

Em `backend/rag.py`, substitua `generate_answer()`:

```python
def generate_answer(question: str, contexts: list[dict]) -> str:
    # Montar prompt
    prompt = f"Pergunta: {question}\n\nContextos:\n"
    for ctx in contexts:
        prompt += f"- {ctx['content']}\n"
    
    # Chamar LLM externo
    # response = call_copilot_api(prompt)
    # response = call_openai_api(prompt)
    # return response
    
    # Por enquanto, placeholder
    return gerar_resposta_placeholder(contexts)

### Feature Flag: Expans√£o de Query via LLM

- Estado atual (jan/2026): DESATIVADA por padr√£o para refor√ßar grounding no acervo.
- Controle via `.env`:

```
ENABLE_LLM_EXPANSION=false  # padr√£o
```

- Comportamento:
    - `false`: apenas `expand_query_with_synonyms()` √© usado
    - `true`: `expand_query_with_llm()` √© habilitado com prompt restritivo e filtros locais
- C√≥digo:
    - `backend/settings.py`: l√™ `ENABLE_LLM_EXPANSION`
    - `backend/rag.py`: `get_query_expander(use_llm=settings.ENABLE_LLM_EXPANSION, use_synonyms=True)`
    - `backend/query_expansion.py`: default `use_llm=settings.ENABLE_LLM_EXPANSION`
```

### Mudar tamanho de chunks

Em `backend/rag.py`, fun√ß√£o `chunk_text()`:

```python
def chunk_text(
    pages: list[str],
    chunk_size: int = 1500,  # Aumentar de 1200
    overlap: int = 200       # Aumentar de 150
) -> list[dict]:
    # ...
```

### Mudar modelo de embedding

Em `.env`:

```
EMBEDDING_MODEL=sentence-transformers/all-mpnet-base-v2
```

Nota: Modelos diferentes t√™m dimensionalidades diferentes. Verifique antes de usar!

### Ajustar TOP_K e MIN_SIM

Em `.env`:

```
TOP_K=10              # Buscar top-10 ao inv√©s de 5
MIN_SIM=0.35          # Aumentar threshold de similaridade
```

## Testing & Debugging

### Verificar sa√∫de do backend

```bash
curl http://localhost:8000/healthz
```

### Acessar documenta√ß√£o autom√°tica

```
http://localhost:8000/docs       # Swagger UI
http://localhost:8000/redoc      # ReDoc
```

### Testar endpoint /ask

```bash
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "O que √© Umbanda?"}'

Logs esperados (com LLM expansion desativada):
- `üîÑ Query Expansion: 1 query ‚Üí N queries` (sin√¥nimos apenas)
- `üîç Dense Search: X resultados √∫nicos`
```

### Verificar √≠ndice FAISS

```python
# Em Python REPL
import faiss
index = faiss.read_index("backend/data/index/index.faiss")
print(f"Vetores no √≠ndice: {index.ntotal}")
```

### Verificar metadados

```python
import json
with open("backend/data/index/metadata.json") as f:
    meta = json.load(f)
    print(f"Docs: {len(meta['documents'])}")
    print(f"Chunks: {len(meta['chunks'])}")
```

## Performance & Optimization

### √çndices FAISS
- `IndexFlatIP`: O mais simples, busca exaustiva (atual)
- `IndexIVF`: Mais r√°pido para grandes datasets
- `IndexHNSW`: Aproximado, muito r√°pido

### Modelos de Embedding
- `all-MiniLM-L6-v2`: 384 dims, leve, r√°pido (atual)
- `all-mpnet-base-v2`: 768 dims, melhor qualidade
- `all-distilroberta-v1`: 768 dims, bom custo-benef√≠cio

### Otimiza√ß√µes
1. Cache do embedder (j√° implementado)
2. Normaliza√ß√£o L2 para cosine similarity
3. Filtro min_sim para reduzir ru√≠do
4. Chunk overlap para continuidade

## Depend√™ncias & Vers√µes

### Backend
- FastAPI 0.115.0 - Framework web
- Uvicorn 0.30.0 - ASGI server
- Pydantic 2.9.2 - Valida√ß√£o
- FAISS 1.8.0 - Busca vetorial
- SentenceTransformers 3.0.1 - Embeddings
- PyMuPDF 1.24.9 - PDF parsing
- Python-dotenv 1.0.1 - .env loader

### Frontend
- React 18.2 - UI library
- TypeScript 5.0 - Type safety
- Vite 5.0 - Build tool
- Tailwind 3.3 - Styling
- TanStack Query 5.0 - State management

## Git Workflow

```bash
# Clone
git clone <repo>
cd umbanda-qa

# Create .env
cp .env.example .env

# Backend setup
python -m venv .venv
.venv\Scripts\activate
pip install -r backend/requirements.txt

# Frontend setup
cd frontend
npm install
cd ..

# Run
# Terminal 1:
uvicorn backend.main:app --reload

# Terminal 2:
cd frontend && npm run dev
```

## Contributing

1. Fork/Clone
2. Create feature branch
3. Make changes
4. Test locally
5. Commit & push
6. Open PR

## Resources

- FastAPI: https://fastapi.tiangolo.com/
- FAISS: https://github.com/facebookresearch/faiss
- SentenceTransformers: https://www.sbert.net/
- React: https://react.dev/
- Vite: https://vitejs.dev/
- Tailwind: https://tailwindcss.com/

## License

MIT (ou conforme preferir)

---

**Last Updated:** Novembro 2025
