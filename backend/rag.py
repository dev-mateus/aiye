"""
M√≥dulo RAG (Retrieval-Augmented Generation).
Respons√°vel por:
  - Extrair texto de PDFs
  - Chunk de texto com overlap
  - Criar e gerenciar √≠ndice FAISS
  - Buscar documentos relevantes
  - Gerar respostas a partir dos contextos recuperados
  - Cache de respostas frequentes
  - Re-ranking de documentos
"""

import json
import os
import uuid
from pathlib import Path
from typing import Optional

import numpy as np
import fitz  # PyMuPDF
import faiss
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
from . import settings
from .cache import get_response_cache
from .reranker import rerank_results


# Cache global para o embedder (evita recarregar m√∫ltiplas vezes)
_embedder: Optional[SentenceTransformer] = None


def load_embedder() -> SentenceTransformer:
    """
    Carrega o modelo de embedding do HuggingFace.
    Utiliza cache global para evitar recarregamento.
    """
    global _embedder
    if _embedder is None:
        print(f"Carregando modelo de embedding: {settings.EMBEDDING_MODEL}")
        _embedder = SentenceTransformer(settings.EMBEDDING_MODEL)
    return _embedder


def extract_text_from_pdf(pdf_path: str) -> list[str]:
    """
    Extrai texto de um PDF usando PyMuPDF (fitz).
    Retorna lista de strings, uma para cada p√°gina.
    """
    pages_text = []
    try:
        pdf_document = fitz.open(pdf_path)
        for page_num in range(len(pdf_document)):
            page = pdf_document[page_num]
            text = page.get_text("text")
            pages_text.append(text)
        pdf_document.close()
        print(f"‚úì Extra√≠do texto de {len(pages_text)} p√°ginas: {pdf_path}")
    except Exception as e:
        print(f"‚úó Erro ao extrair texto de {pdf_path}: {e}")
    return pages_text


def chunk_text(
    pages: list[str],
    chunk_size: int = 1500,
    overlap: int = 200
) -> list[dict]:
    """
    Divide o texto em chunks com overlap.
    Cada chunk mant√©m refer√™ncia √† p√°gina inicial e final.
    
    Retorna lista de dicts: {"content": str, "page_start": int, "page_end": int}
    """
    chunks = []
    
    # Combina todas as p√°ginas em um √∫nico texto, mas mant√©m track de p√°ginas
    full_text = ""
    page_ranges = []  # Tuples (char_start, char_end, page_num)
    char_count = 0
    
    for page_num, page_text in enumerate(pages):
        start = char_count
        full_text += page_text + "\n\n"
        char_count = len(full_text)
        end = char_count
        page_ranges.append((start, end, page_num))
    
    # Cria chunks com overlap
    position = 0
    while position < len(full_text):
        chunk_end = min(position + chunk_size, len(full_text))
        chunk_content = full_text[position:chunk_end].strip()
        
        if chunk_content:
            # Determina p√°gina inicial e final
            page_start = 0
            page_end = len(pages) - 1
            for start, end, page_num in page_ranges:
                if start <= position < end:
                    page_start = page_num
                if start <= chunk_end - 1 < end:
                    page_end = page_num
            
            chunks.append({
                "content": chunk_content,
                "page_start": page_start + 1,  # 1-indexed
                "page_end": page_end + 1,
            })
        
        position += chunk_size - overlap
    
    return chunks


def load_or_create_index(index_dir: str) -> tuple[faiss.IndexFlatIP, dict]:
    """
    Carrega √≠ndice FAISS existente ou cria um novo.
    Tamb√©m carrega metadados do JSON.
    
    Retorna: (faiss_index, metadata_dict)
    """
    index_path = os.path.join(index_dir, "index.faiss")
    metadata_path = os.path.join(index_dir, "metadata.json")
    
    # Carrega metadados
    metadata = {"documents": [], "chunks": []}
    if os.path.exists(metadata_path):
        try:
            with open(metadata_path, "r", encoding="utf-8") as f:
                metadata = json.load(f)
            print(f"‚úì Metadados carregados: {len(metadata.get('documents', []))} docs, {len(metadata.get('chunks', []))} chunks")
        except Exception as e:
            print(f"‚úó Erro ao carregar metadados: {e}")
    
    # Carrega ou cria √≠ndice FAISS
    if os.path.exists(index_path):
        try:
            faiss_index = faiss.read_index(index_path)
            print(f"‚úì √çndice FAISS carregado: {faiss_index.ntotal} vetores")
        except Exception as e:
            print(f"‚úó Erro ao carregar √≠ndice FAISS: {e}")
            faiss_index = faiss.IndexFlatIP(384)  # all-MiniLM-L6-v2 tem 384 dimens√µes
    else:
        # Cria novo √≠ndice
        faiss_index = faiss.IndexFlatIP(384)  # all-MiniLM-L6-v2 tem 384 dimens√µes
        print("‚úì Novo √≠ndice FAISS criado (384 dimens√µes)")
    
    return faiss_index, metadata


def save_index_and_metadata(
    faiss_index: faiss.IndexFlatIP,
    metadata: dict,
    index_dir: str
) -> None:
    """Salva o √≠ndice FAISS e os metadados em disco."""
    os.makedirs(index_dir, exist_ok=True)
    
    # Salva √≠ndice FAISS
    index_path = os.path.join(index_dir, "index.faiss")
    faiss.write_index(faiss_index, index_path)
    print(f"‚úì √çndice FAISS salvo: {index_path}")
    
    # Salva metadados
    metadata_path = os.path.join(index_dir, "metadata.json")
    with open(metadata_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    print(f"‚úì Metadados salvos: {metadata_path}")


def add_document_to_index(
    pdf_path: str,
    title: str,
    index_dir: str = settings.INDEX_DIR,
    embedder: Optional[SentenceTransformer] = None
) -> None:
    """
    Extrai texto de um PDF, cria chunks, embed e adiciona ao √≠ndice FAISS.
    Atualiza os metadados em JSON.
    """
    embedder = embedder or load_embedder()
    
    # Extrai texto
    pages = extract_text_from_pdf(pdf_path)
    if not pages:
        return
    
    # Cria chunks
    chunks = chunk_text(pages)
    print(f"  ‚Üí {len(chunks)} chunks criados")
    
    # Carrega √≠ndice e metadados
    faiss_index, metadata = load_or_create_index(index_dir)
    
    # Cria documento
    doc_id = str(uuid.uuid4())
    source_uri = pdf_path
    doc_metadata = {
        "document_id": doc_id,
        "title": title,
        "source_uri": source_uri,
        "pages": len(pages)
    }
    metadata["documents"].append(doc_metadata)
    
    # Processa chunks
    embeddings_list = []
    for chunk in chunks:
        chunk_id = str(uuid.uuid4())
        
        # Embed
        embedding = embedder.encode(chunk["content"], convert_to_numpy=True)
        embeddings_list.append(embedding)
        
        # Salva chunk em metadados
        chunk_metadata = {
            "document_id": doc_id,
            "chunk_id": chunk_id,
            "page_start": chunk["page_start"],
            "page_end": chunk["page_end"],
            "content": chunk["content"]
        }
        metadata["chunks"].append(chunk_metadata)
    
    # Adiciona embeddings ao √≠ndice
    if embeddings_list:
        embeddings_array = np.array(embeddings_list, dtype=np.float32)
        faiss.normalize_L2(embeddings_array)
        faiss_index.add(embeddings_array)  # type: ignore
        print(f"  ‚Üí {len(embeddings_list)} embeddings adicionados ao √≠ndice")
    
    # Salva √≠ndice e metadados
    save_index_and_metadata(faiss_index, metadata, index_dir)


def search(
    query: str,
    top_k: int = 8,
    min_sim: float = 0.30,
    index_dir: str = settings.INDEX_DIR,
    embedder: Optional[SentenceTransformer] = None,
    use_reranking: bool = True
) -> list[dict]:
    """
    Busca chunks relevantes para a query no √≠ndice FAISS.
    Opcionalmente aplica re-ranking para melhorar relev√¢ncia.
    
    Args:
        query: Pergunta do usu√°rio
        top_k: N√∫mero de resultados a retornar
        min_sim: Similaridade m√≠nima (0-1)
        index_dir: Diret√≥rio do √≠ndice FAISS
        embedder: Modelo de embedding (opcional)
        use_reranking: Se True, aplica re-ranking aos resultados
    
    Returns:
        Lista de dicts com contextos relevantes ordenados por relev√¢ncia
    """
    embedder = embedder or load_embedder()
    
    # Carrega √≠ndice e metadados
    faiss_index, metadata = load_or_create_index(index_dir)
    
    if faiss_index.ntotal == 0:
        return []
    
    # Embed a query
    query_embedding = embedder.encode(query, convert_to_numpy=True)
    query_embedding = np.array([query_embedding], dtype=np.float32)
    faiss.normalize_L2(query_embedding)
    
    # Busca no FAISS
    distances, indices = faiss_index.search(query_embedding, int(top_k))  # type: ignore
    distances = distances[0]
    indices = indices[0]
    
    # Constr√≥i resultados
    chunks_metadata = metadata.get("chunks", [])
    docs_metadata = {doc["document_id"]: doc for doc in metadata.get("documents", [])}
    
    print(f"üîç Busca: '{query}' | Top-{top_k} | min_sim={min_sim}")
    print(f"   Scores retornados: {distances.tolist()}")
    print(f"   √çndices retornados: {indices.tolist()}")
    print(f"   Total de chunks em metadata: {len(chunks_metadata)}")
    
    results = []
    print(f"   Iniciando loop com {len(list(zip(distances, indices)))} items")
    
    for i, (distance, idx) in enumerate(zip(distances, indices)):
        print(f"   Loop {i}: idx={idx}, distance={distance}")
        
        if idx < 0 or idx >= len(chunks_metadata):
            print(f"   ‚ùå √çndice {idx} fora do range (total chunks: {len(chunks_metadata)})")
            continue
        
        # Similarity score (FAISS IndexFlatIP retorna produto interno normalizado)
        score = float(distance)
        
        print(f"   Chunk {idx}: score={score:.4f} (min={min_sim})")
        
        if score < min_sim:
            print(f"   ‚ùå Filtrado por score baixo")
            continue
        
        chunk_meta = chunks_metadata[idx]
        doc_id = chunk_meta["document_id"]
        doc_meta = docs_metadata.get(doc_id, {})
        
        result = {
            "content": chunk_meta["content"],
            "title": doc_meta.get("title", "Unknown"),
            "page_start": chunk_meta["page_start"],
            "page_end": chunk_meta["page_end"],
            "uri": doc_meta.get("source_uri", ""),
            "score": score
        }
        results.append(result)
        print(f"   ‚úÖ Adicionado: {doc_meta.get('title', 'Unknown')[:40]}... (p√°g {chunk_meta['page_start']})")
    
    print(f"   üìä Total de resultados retornados: {len(results)}")
    
    # Aplica re-ranking se habilitado
    if use_reranking and results:
        results = rerank_results(query, results)
    
    return results


def generate_answer(question: str, contexts: list[dict]) -> str:
    """
    Gera uma resposta coerente e sintetizada usando Google Gemini.
    
    Estrat√©gia:
    1. Se n√£o houver contextos, avisa que precisa consultar dirigente
    2. Se houver contextos, envia para Gemini sintetizar uma resposta
    3. Gemini gera resposta em portugu√™s, bem estruturada
    4. Adiciona cita√ß√µes de fontes (documentos e p√°ginas)
    
    Integra√ß√£o: Google Generative AI (Gemini) - modelo de ponta para portugu√™s
    """
    if not contexts:
        return "N√£o encontrei essa informa√ß√£o no acervo, entre em contato com o administrador da plataforma."
    
    try:
        # Configura Gemini com a API key
        if not settings.GOOGLE_API_KEY:
            return "‚ö†Ô∏è Erro: GOOGLE_API_KEY n√£o configurada. Por favor, defina a vari√°vel de ambiente."
        
        genai.configure(api_key=settings.GOOGLE_API_KEY)
        model = genai.GenerativeModel("gemini-2.5-flash")
        
        # Monta contexto para Gemini (combina todos os chunks com fontes)
        context_text = "CONTEXTOS DO ACERVO DE UMBANDA:\n\n"
        sources = set()
        
        for ctx in contexts:
            title = ctx.get("title", "Desconhecido")
            page_start = ctx.get("page_start", "?")
            page_end = ctx.get("page_end", "?")
            content = ctx.get("content", "").strip()
            
            context_text += f"[{title} - pp. {page_start}-{page_end}]\n{content}\n\n"
            sources.add(f"{title} (pp. {page_start}-{page_end})")
        
        # Prompt otimizado e estruturado
        prompt = f"""Voc√™ √© um especialista em Umbanda com profundo conhecimento sobre suas tradi√ß√µes, fundamentos e pr√°ticas.

CONTEXTOS DISPON√çVEIS:
{context_text}

PERGUNTA DO USU√ÅRIO:
{question}

INSTRU√á√ïES DETALHADAS:
1. Analise cuidadosamente os contextos fornecidos acima
2. Responda APENAS com informa√ß√µes que est√£o explicitamente presentes nos contextos
3. Se a informa√ß√£o for insuficiente, vaga ou n√£o relacionada √† pergunta, responda exatamente: "N√ÉO_ENCONTREI"
4. Organize sua resposta de forma clara e estruturada:
   - Use par√°grafos curtos para facilitar a leitura
   - Se houver m√∫ltiplos pontos, use t√≥picos numerados ou marcadores
   - Destaque conceitos importantes quando relevante
5. Seja preciso e objetivo, mas completo na explica√ß√£o
6. Sempre respeite as varia√ß√µes entre diferentes terreiros e tradi√ß√µes
7. Use linguagem acess√≠vel, evitando jarg√µes excessivos sem explica√ß√£o
8. N√ÉO invente informa√ß√µes que n√£o estejam nos contextos
9. N√ÉO cite os documentos ou p√°ginas na resposta (isso ser√° feito automaticamente)
10. Se a resposta envolver pr√°ticas ritual√≠sticas, lembre que podem variar

FORMATO DA RESPOSTA:
- Seja direto e informativo
- Use portugu√™s brasileiro claro
- Estruture com par√°grafos ou t√≥picos quando apropriado
- Mantenha tom respeitoso e educativo

RESPOSTA COMPLETA:"""
        
        # Chama Gemini
        response = model.generate_content(prompt)
        answer = response.text.strip()
        
        # Se Gemini indicou que n√£o encontrou, retorna a mensagem padr√£o
        if "N√ÉO_ENCONTREI" in answer.upper():
            return "N√£o encontrei essa informa√ß√£o no acervo, entre em contato com o administrador da plataforma."
        
        # Retorna apenas a resposta do Gemini
        # As fontes e avisos s√£o exibidos pelo frontend no card SourceList
        return answer
        
    except Exception as e:
        print(f"Erro ao chamar Gemini: {e}")
        # Fallback: resposta simples se falhar
        return (
            f"Desculpe, ocorreu um erro ao processar sua pergunta: {str(e)}. "
            "Por favor, consulte um dirigente ou tente novamente mais tarde."
        )


def ask_with_cache(
    question: str,
    top_k: int = 8,
    min_sim: float = 0.30,
    use_cache: bool = True,
    use_reranking: bool = True,
    index_dir: str = None
) -> tuple[str, list[dict]]:
    """
    Fun√ß√£o principal que integra cache, busca, re-ranking e gera√ß√£o de resposta.
    
    Args:
        question: Pergunta do usu√°rio
        top_k: N√∫mero de documentos a recuperar
        min_sim: Similaridade m√≠nima
        use_cache: Se True, usa cache de respostas
        use_reranking: Se True, aplica re-ranking
        index_dir: Diret√≥rio do √≠ndice (opcional, usa settings.INDEX_DIR se None)
    
    Returns:
        Tupla (resposta, contextos)
    """
    if index_dir is None:
        index_dir = settings.INDEX_DIR
    
    cache = get_response_cache()
    
    # Tenta recuperar do cache
    if use_cache:
        cached = cache.get(question)
        if cached:
            return cached['answer'], cached['contexts']
    
    # Cache miss: busca + gera resposta
    contexts = search(
        query=question,
        top_k=top_k,
        min_sim=min_sim,
        index_dir=index_dir,
        use_reranking=use_reranking
    )
    
    answer = generate_answer(question, contexts)
    
    # Armazena no cache
    if use_cache:
        cache.set(question, answer, contexts)
    
    return answer, contexts
