---
title: Aiye
emoji: ðŸŒ¿
colorFrom: green
colorTo: blue
sdk: docker
app_port: 7860
pinned: false
license: mit
short_description: Plataforma RAG para perguntas sobre Umbanda
---

# Aiye â€“ Plataforma de Perguntas sobre Umbanda

Plataforma **RAG (Retrieval-Augmented Generation)** para responder perguntas sobre Umbanda, Espiritismo e temas afins utilizando inteligÃªncia artificial, embeddings vetoriais e LLM via Groq (endpoint OpenAI-compatible).

> AtualizaÃ§Ã£o (jan/2026): A expansÃ£o de consultas via LLM foi desativada por padrÃ£o para reforÃ§ar o grounding no acervo e evitar dependÃªncias quebradas. A busca continua usando sinÃ´nimos controlados do domÃ­nio de Umbanda. Ã‰ possÃ­vel reativar com a flag `ENABLE_LLM_EXPANSION=true` no `.env`.

## ðŸŽ¯ Objetivo

Criar um espaÃ§o de conhecimento onde perguntas sobre Umbanda sÃ£o respondidas com base em um acervo curado de PDFs. As respostas sÃ£o geradas por IA e sempre citam as fontes consultadas, respeitando as variaÃ§Ãµes entre diferentes terreiros e tradiÃ§Ãµes.

## âš™ï¸ Requisitos

**Desenvolvimento Local:**
- **Python 3.11+** (backend)
- **Node.js 18+** (frontend)
- **Groq API Key** (obrigatÃ³rio)
- **Google API Key** (opcional, para fallback futuro)
- ~2 GB de espaÃ§o em disco (para modelos de embedding e Ã­ndices FAISS)

**ProduÃ§Ã£o:**
- Conta Hugging Face (backend)
- Conta Vercel (frontend)
- Git LFS configurado (para PDFs e Ã­ndices)

## ðŸš€ Como Rodar

### Backend

1. **Criar ambiente virtual:**
   ```bash
   python -m venv .venv
   # Windows:
   .venv\Scripts\activate
   # macOS/Linux:
   source .venv/bin/activate
   ```

2. **Instalar dependÃªncias:**
   ```bash
   pip install -r backend/requirements.txt
   ```

3. **Criar arquivo `.env` (copiar de `.env.example`):**
   ```bash
   cp .env.example .env
   ```

4. **Definir `GROQ_API_KEY` no `.env`**

5. **Ingerir PDFs:**
   - Coloque os PDFs em `backend/data/pdfs/`
   - Execute:
     ```bash
     python backend/ingest.py
     ```
   - Isso gerarÃ¡ `backend/data/index/index.faiss` e `metadata.json`

5. **Iniciar servidor:**
   ```bash
   uvicorn backend.main:app --reload --port 8000
   ```
   - Acesse: `http://localhost:8000`
   - Docs interativa: `http://localhost:8000/docs`

### Frontend

1. **Instalar dependÃªncias:**
   ```bash
   cd frontend
   npm install
   ```

2. **Criar `.env.local`:**
   ```
   VITE_API_BASE=http://localhost:8000
   ```

3. **Iniciar servidor de desenvolvimento:**
   ```bash
   npm run dev
   ```
   - Acesse: `http://localhost:5173`

## ðŸ“‹ Estrutura de Pastas

```
aiye/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ ingest.py            # Script de ingestÃ£o de PDFs
â”‚   â”œâ”€â”€ rag.py               # LÃ³gica de RAG (embedding, search, answer generation)
â”‚   â”œâ”€â”€ models.py            # Modelos Pydantic
â”‚   â”œâ”€â”€ settings.py          # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ requirements.txt      # DependÃªncias Python
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ pdfs/            # PDFs para ingestÃ£o
â”‚       â””â”€â”€ index/           # Ãndices FAISS e metadados
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.tsx         # Entrada React
â”‚   â”‚   â”œâ”€â”€ App.tsx          # Componente principal
â”‚   â”‚   â”œâ”€â”€ api.ts           # Cliente HTTP
â”‚   â”‚   â”œâ”€â”€ styles.css       # Estilos Tailwind
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â”œâ”€â”€ ChatBox.tsx      # Input e botÃ£o
â”‚   â”‚       â”œâ”€â”€ AnswerCard.tsx   # Resposta
â”‚   â”‚       â””â”€â”€ SourceList.tsx   # Fontes
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â”œâ”€â”€ postcss.config.js
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ðŸ“– Fluxo de Uso

1. **IngestÃ£o de PDFs:** Execute `python backend/ingest.py` para processar PDFs em `backend/data/pdfs/` e gerar o Ã­ndice FAISS
2. **Fazer pergunta:** Digite sua pergunta no frontend (https://aiye-chat.vercel.app)
3. **Receber resposta:** O backend busca chunks relevantes no Ã­ndice FAISS, envia para o Gemini sintetizar uma resposta coerente e retorna com as fontes consultadas
4. **Consultar fontes:** Visualize os documentos consultados (sem download de PDFs por questÃµes de direitos autorais)

## ðŸ§  Como Funciona o RAG

- **Embeddings:** `sentence-transformers/all-MiniLM-L6-v2` gera vetores de 384 dimensÃµes para cada chunk de texto
- **Ãndice:** FAISS (IndexFlatIP) armazena os embeddings para busca eficiente por similaridade
- **Chunking:** PDFs divididos em chunks de 1500 caracteres com overlap de 200 para manter contexto
- **Busca:** Similaridade de cosseno entre a pergunta embedada e os chunks do acervo (top-8, threshold 0.30)
- **GeraÃ§Ã£o:** LLM via Groq (cliente OpenAI) sintetiza a resposta final baseada nos contextos recuperados
- **Metadados:** JSON com informaÃ§Ãµes sobre documentos, chunks, pÃ¡ginas e scores de relevÃ¢ncia

## ðŸ¤– IntegraÃ§Ã£o com Google Gemini
## ðŸ¤– IntegraÃ§Ã£o com Groq (OpenAI-compatible)

O backend usa o cliente OpenAI apontando para o endpoint Groq:

- Configure `GROQ_API_KEY` no `.env` (dev) ou em Repository Secrets (HF Spaces)
- VariÃ¡veis suportadas: `GROQ_API_KEY`, `GROQ_MODEL`, `GROQ_BASE_URL`
- Prompt reforÃ§a: â€œReformule usando APENAS os contextos. NÃ£o invente informaÃ§Ã£o.â€

Gemini permanece opcional para futuro fallback (via `GOOGLE_API_KEY`).

### ExpansÃ£o de Query (LLM) â€“ Estado Atual
- Por padrÃ£o estÃ¡ DESATIVADA para garantir respostas estritamente baseadas no acervo.
- Somente sinÃ´nimos do domÃ­nio sÃ£o usados para expandir queries (ex.: `orixÃ¡` â†’ `orishas`, `divindades`).
- Para reativar: defina `ENABLE_LLM_EXPANSION=true` no `.env`. A lÃ³gica usa um prompt restritivo e aplica filtros locais para evitar drift.

## âš ï¸ Aviso Ã‰tico

- Este sistema Ã© um **complemento informativo**, nÃ£o substitui orientaÃ§Ã£o de um dirigente espiritual
- As tradiÃ§Ãµes da Umbanda **variam** entre terreiros e regiÃµes
- Sempre cite as fontes e recomende consultar um dirigente para questÃµes especÃ­ficas
- O conteÃºdo ingerido deve ser confiÃ¡vel e autorizado

## ðŸš€ Deploy em ProduÃ§Ã£o

### Backend (Hugging Face Spaces)
- **URL ProduÃ§Ã£o:** https://dev-mateus-backend-aiye.hf.space
- **Tecnologia:** Docker (Python 3.11-slim) com FastAPI + Uvicorn
- **Deploy:** AutomÃ¡tico via `git push space main`
- **Armazenamento:** PDFs e Ã­ndice FAISS via **Git LFS** (metadata.json ~22MB, index.faiss ~133KB)
- **Build:** Dockerfile executa `backend/init_index.py` para gerar Ã­ndice se nÃ£o existir
- **Secrets:** `GOOGLE_API_KEY` configurada em Repository secrets

**Comandos de deploy:**
```bash
git add .
git commit -m "mensagem"
git push origin main   # GitHub
git push space main    # Hugging Face Spaces (trigger rebuild)
```

Ver guia completo em [`DEPLOY_HUGGINGFACE.md`](./DEPLOY_HUGGINGFACE.md)

### Frontend (Vercel)
- **URL ProduÃ§Ã£o:** https://aiye-chat.vercel.app
- **Tecnologia:** React 18 + TypeScript + Vite 5 + Tailwind CSS 3
- **Deploy:** AutomÃ¡tico via GitHub (branch `main`)
- **VariÃ¡vel de Ambiente:** `VITE_API_BASE=https://dev-mateus-backend-aiye.hf.space`
- **Build:** Vite build com TypeScript check a cada push

### Arquitetura de Deploy
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTPS/JSON      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Vercel      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚  Hugging Face      â”‚
â”‚   (Frontend)    â”‚                      â”‚  Spaces (Backend)  â”‚
â”‚ React+Vite+TS   â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  FastAPI+Docker    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                   â”‚
                                                   â”œâ”€ FAISS Index (LFS)
                                                   â”œâ”€ PDFs (LFS)  
                                                   â”œâ”€ Sentence Transformers
                                                   â””â”€ Groq API (OpenAI-compatible)
```

## ðŸ“¦ DependÃªncias

### Backend
- **FastAPI 0.115.0** - Framework web moderno e rÃ¡pido
- **Uvicorn 0.30.0** - Servidor ASGI de alta performance
- **FAISS 1.13.0** - Busca vetorial eficiente (CPU)
- **Sentence Transformers 3.0.1** - GeraÃ§Ã£o de embeddings
- **PyMuPDF 1.24.9** - Parsing e extraÃ§Ã£o de texto de PDFs
- **OpenAI 1.55.3** - Cliente OpenAI (endpoint Groq)
- **Pydantic 2.x** - ValidaÃ§Ã£o de dados

### Frontend
- **React 18.2** - Biblioteca UI declarativa
- **TypeScript 5.0** - Type safety
- **Vite 5.0** - Build tool ultrarrÃ¡pido
- **Tailwind CSS 3.3** - Framework CSS utility-first
- **Axios** - Cliente HTTP

## ðŸ¤ Contribuindo

Este Ã© um projeto educacional e informativo. Para contribuir:

1. FaÃ§a fork do repositÃ³rio
2. Crie uma branch para sua feature (`git checkout -b feature/nova-feature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add: nova feature'`)
4. Push para a branch (`git push origin feature/nova-feature`)
5. Abra um Pull Request

**SugestÃµes de contribuiÃ§Ã£o:**
- Adicionar novos PDFs ao acervo (com direitos autorais respeitados)
- Melhorar o prompt do Gemini para respostas mais precisas
- Implementar feature de visualizaÃ§Ã£o de trechos dos PDFs (similar ao Google Books)
- Adicionar suporte a outros idiomas
- Melhorar o design do frontend

## ðŸ“„ LicenÃ§a

MIT License - veja o arquivo LICENSE para detalhes.

## ðŸ‘¨â€ðŸ’» Autor

Desenvolvido com â¤ï¸ por [Mateus](https://github.com/dev-mateus)

---

**Status:** âœ… Em produÃ§Ã£o  
**VersÃ£o:** 1.0.0  
**Ãšltima atualizaÃ§Ã£o:** Novembro 2025
