---
title: Aiye
emoji: ðŸŒ¿
colorFrom: green
colorTo: blue
sdk: docker
app_port: 7860
pinned: false
license: mit
short_description: Plataforma RAG para perguntas sobre Umbanda
---

# Aiye â€“ Plataforma de Perguntas sobre Umbanda

Uma plataforma **local-first** para responder perguntas sobre Umbanda utilizando **RAG (Retrieval-Augmented Generation)** com embeddings vetoriais e integraÃ§Ã£o com Google Gemini.

## ðŸŽ¯ Objetivo

Criar um espaÃ§o de conhecimento colaborativo onde perguntas sobre Umbanda sÃ£o respondidas com base em um acervo de PDFs. As respostas sÃ£o sempre citadas com as fontes, respeitando as variaÃ§Ãµes entre diferentes terreiros e tradiÃ§Ãµes.

## âš™ï¸ Requisitos

- **Python 3.11+** (backend)
- **Node.js 18+** (frontend)
- ~2 GB de espaÃ§o em disco (para modelos de embedding e Ã­ndices FAISS)

## ðŸš€ Como Rodar

### Backend

1. **Criar ambiente virtual:**
   ```bash
   python -m venv .venv
   # Windows:
   .venv\Scripts\activate
   # macOS/Linux:
   source .venv/bin/activate
   ```

2. **Instalar dependÃªncias:**
   ```bash
   pip install -r backend/requirements.txt
   ```

3. **Criar arquivo `.env` (copiar de `.env.example`):**
   ```bash
   cp .env.example .env
   ```

4. **Ingerir PDFs:**
   - Coloque os PDFs em `backend/data/pdfs/`
   - Execute:
     ```bash
     python backend/ingest.py
     ```
   - Isso gerarÃ¡ `backend/data/index/index.faiss` e `metadata.json`

5. **Iniciar servidor:**
   ```bash
   uvicorn backend.main:app --reload --port 8000
   ```
   - Acesse: `http://localhost:8000`
   - Docs interativa: `http://localhost:8000/docs`

### Frontend

1. **Instalar dependÃªncias:**
   ```bash
   cd frontend
   npm install
   ```

2. **Criar `.env.local`:**
   ```
   VITE_API_BASE=http://localhost:8000
   ```

3. **Iniciar servidor de desenvolvimento:**
   ```bash
   npm run dev
   ```
   - Acesse: `http://localhost:5173`

## ðŸ“‹ Estrutura de Pastas

```
aiye/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ ingest.py            # Script de ingestÃ£o de PDFs
â”‚   â”œâ”€â”€ rag.py               # LÃ³gica de RAG (embedding, search, answer generation)
â”‚   â”œâ”€â”€ models.py            # Modelos Pydantic
â”‚   â”œâ”€â”€ settings.py          # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ requirements.txt      # DependÃªncias Python
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ pdfs/            # PDFs para ingestÃ£o
â”‚       â””â”€â”€ index/           # Ãndices FAISS e metadados
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.tsx         # Entrada React
â”‚   â”‚   â”œâ”€â”€ App.tsx          # Componente principal
â”‚   â”‚   â”œâ”€â”€ api.ts           # Cliente HTTP
â”‚   â”‚   â”œâ”€â”€ styles.css       # Estilos Tailwind
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â”œâ”€â”€ ChatBox.tsx      # Input e botÃ£o
â”‚   â”‚       â”œâ”€â”€ AnswerCard.tsx   # Resposta
â”‚   â”‚       â””â”€â”€ SourceList.tsx   # Fontes
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â”œâ”€â”€ postcss.config.js
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ðŸ“– Fluxo de Uso

1. **Ingerir PDFs:** Execute `python backend/ingest.py` para processar PDFs em `backend/data/pdfs/`
2. **Fazer pergunta:** Digite no textarea do frontend
3. **Receber resposta:** O backend busca chunks relevantes no Ã­ndice FAISS, gera uma resposta coerente e lista as fontes
4. **Consultar fontes:** Links para os PDFs originais

## ðŸ§  Como Funciona o RAG

- **Embeddings:** Utilizamos `sentence-transformers/all-MiniLM-L6-v2` para gerar embeddings vetoriais (384 dimensÃµes)
- **Ãndice:** FAISS (CPU) armazena os embeddings localmente
- **Busca:** Busca coseno-similarity entre a pergunta e os chunks do acervo
- **Resposta:** Placeholder que gera uma resposta a partir dos contextos recuperados (sem LLM externo)
- **Metadados:** JSON com informaÃ§Ãµes sobre documentos e chunks

## ðŸ¤– IntegraÃ§Ã£o com Google Gemini

O projeto usa **Google Gemini 2.5 Flash** para gerar respostas inteligentes baseadas nos contextos recuperados:

- Configure `GOOGLE_API_KEY` no arquivo `.env` ou nas variÃ¡veis de ambiente do deploy
- O modelo sintetiza informaÃ§Ãµes dos PDFs em respostas coerentes e bem estruturadas
- Respostas incluem citaÃ§Ãµes das fontes e avisos sobre variaÃ§Ãµes regionais da Umbanda

## âš ï¸ Aviso Ã‰tico

- Este sistema Ã© um **complemento informativo**, nÃ£o substitui orientaÃ§Ã£o de um dirigente espiritual
- As tradiÃ§Ãµes da Umbanda **variam** entre terreiros e regiÃµes
- Sempre cite as fontes e recomende consultar um dirigente para questÃµes especÃ­ficas
- O conteÃºdo ingerido deve ser confiÃ¡vel e autorizado

## ðŸš€ Deploy em ProduÃ§Ã£o

### Backend (Hugging Face Spaces)
- **URL:** https://dev-mateus-backend-aiye.hf.space
- Deploy automÃ¡tico via Git push para branch `main`
- Usa **Docker SDK** com porta 7860
- PDFs e Ã­ndices FAISS armazenados via **Git LFS**
- Configurar `GOOGLE_API_KEY` nas Repository secrets do Space

**Para fazer deploy:**
```bash
git push space main
```

Ver guia completo em [`DEPLOY_HUGGINGFACE.md`](./DEPLOY_HUGGINGFACE.md)

### Frontend (Vercel)
- **URL:** https://aiye.vercel.app
- Deploy automÃ¡tico via GitHub (branch `main`)
- Configurar `VITE_API_BASE=https://dev-mateus-backend-aiye.hf.space`
- Build automÃ¡tico com Vite a cada push

### Arquitetura de Deploy
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTPS/JSON      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vercel    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ Hugging Face     â”‚
â”‚  (Frontend) â”‚                      â”‚ Spaces (Backend) â”‚
â”‚  React+Vite â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  FastAPI+Docker  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â”œâ”€ FAISS Index (LFS)
                                              â”œâ”€ PDFs (LFS)
                                              â””â”€ Gemini API
```

## ðŸ“¦ DependÃªncias

### Backend
- FastAPI, Uvicorn
- FAISS (busca vetorial)
- Sentence Transformers (embeddings)
- PyMuPDF (parsing de PDFs)
- Pydantic (validaÃ§Ã£o)

### Frontend
- React, React DOM
- Vite (bundler)
- TypeScript
- Tailwind CSS
- TanStack Query (gerenciamento de estado)

## ðŸ¤ Contribuindo

1. Ingira novos PDFs em `backend/data/pdfs/`
2. Execute `python backend/ingest.py` para atualizar o Ã­ndice
3. Envie feedback e melhore a plataforma

## ðŸ“„ LicenÃ§a

MIT (ou conforme vocÃª preferir)

---

**Status:** MVP local-first, sem Docker, sem serviÃ§os pagos.
