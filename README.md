# Umbanda QA â€“ Plataforma de Tira-DÃºvidas Baseada em PDFs

Uma plataforma **local-first**, sem dependÃªncias externas de LLM ou banco de dados, para responder perguntas sobre Umbanda utilizando **RAG (Retrieval-Augmented Generation)** com embeddings vetoriais locais.

## ğŸ¯ Objetivo

Criar um espaÃ§o de conhecimento colaborativo onde perguntas sobre Umbanda sÃ£o respondidas com base em um acervo de PDFs. As respostas sÃ£o sempre citadas com as fontes, respeitando as variaÃ§Ãµes entre diferentes terreiros e tradiÃ§Ãµes.

## âš™ï¸ Requisitos

- **Python 3.11+** (backend)
- **Node.js 18+** (frontend)
- ~2 GB de espaÃ§o em disco (para modelos de embedding e Ã­ndices FAISS)

## ğŸš€ Como Rodar

### Backend

1. **Criar ambiente virtual:**
   ```bash
   python -m venv .venv
   # Windows:
   .venv\Scripts\activate
   # macOS/Linux:
   source .venv/bin/activate
   ```

2. **Instalar dependÃªncias:**
   ```bash
   pip install -r backend/requirements.txt
   ```

3. **Criar arquivo `.env` (copiar de `.env.example`):**
   ```bash
   cp .env.example .env
   ```

4. **Ingerir PDFs:**
   - Coloque os PDFs em `backend/data/pdfs/`
   - Execute:
     ```bash
     python backend/ingest.py
     ```
   - Isso gerarÃ¡ `backend/data/index/index.faiss` e `metadata.json`

5. **Iniciar servidor:**
   ```bash
   uvicorn backend.main:app --reload --port 8000
   ```
   - Acesse: `http://localhost:8000`
   - Docs interativa: `http://localhost:8000/docs`

### Frontend

1. **Instalar dependÃªncias:**
   ```bash
   cd frontend
   npm install
   ```

2. **Criar `.env.local`:**
   ```
   VITE_API_BASE=http://localhost:8000
   ```

3. **Iniciar servidor de desenvolvimento:**
   ```bash
   npm run dev
   ```
   - Acesse: `http://localhost:5173`

## ğŸ“‹ Estrutura de Pastas

```
umbanda-qa/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ ingest.py            # Script de ingestÃ£o de PDFs
â”‚   â”œâ”€â”€ rag.py               # LÃ³gica de RAG (embedding, search, answer generation)
â”‚   â”œâ”€â”€ models.py            # Modelos Pydantic
â”‚   â”œâ”€â”€ settings.py          # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ requirements.txt      # DependÃªncias Python
â”‚   â””â”€â”€ data/
â”‚       â”œâ”€â”€ pdfs/            # PDFs para ingestÃ£o
â”‚       â””â”€â”€ index/           # Ãndices FAISS e metadados
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.tsx         # Entrada React
â”‚   â”‚   â”œâ”€â”€ App.tsx          # Componente principal
â”‚   â”‚   â”œâ”€â”€ api.ts           # Cliente HTTP
â”‚   â”‚   â”œâ”€â”€ styles.css       # Estilos Tailwind
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â”œâ”€â”€ ChatBox.tsx      # Input e botÃ£o
â”‚   â”‚       â”œâ”€â”€ AnswerCard.tsx   # Resposta
â”‚   â”‚       â””â”€â”€ SourceList.tsx   # Fontes
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â”œâ”€â”€ postcss.config.js
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸ“– Fluxo de Uso

1. **Ingerir PDFs:** Execute `python backend/ingest.py` para processar PDFs em `backend/data/pdfs/`
2. **Fazer pergunta:** Digite no textarea do frontend
3. **Receber resposta:** O backend busca chunks relevantes no Ã­ndice FAISS, gera uma resposta coerente e lista as fontes
4. **Consultar fontes:** Links para os PDFs originais

## ğŸ§  Como Funciona o RAG

- **Embeddings:** Utilizamos `sentence-transformers/all-MiniLM-L6-v2` para gerar embeddings vetoriais (384 dimensÃµes)
- **Ãndice:** FAISS (CPU) armazena os embeddings localmente
- **Busca:** Busca coseno-similarity entre a pergunta e os chunks do acervo
- **Resposta:** Placeholder que gera uma resposta a partir dos contextos recuperados (sem LLM externo)
- **Metadados:** JSON com informaÃ§Ãµes sobre documentos e chunks

## ğŸ”§ IntegraÃ§Ã£o com LLM Futuro

O arquivo `backend/rag.py` contÃ©m a funÃ§Ã£o `generate_answer()`, que atualmente Ã© um placeholder. Para integrar com um LLM externo (Copilot, M365, OpenAI, etc.), basta substituir a implementaÃ§Ã£o interna e adicionar a chamada Ã  API:

```python
def generate_answer(question: str, contexts: list[dict]) -> str:
    # TODO: Integrar com Copilot/M365 ou outra API de LLM
    # prompt = f"Responda baseado nos contextos abaixo:\n\n{contextos}\n\nPergunta: {question}"
    # return call_to_llm_api(prompt)
    
    # Por enquanto: gera resposta a partir dos contextos
    ...
```

## âš ï¸ Aviso Ã‰tico

- Este sistema Ã© um **complemento informativo**, nÃ£o substitui orientaÃ§Ã£o de um dirigente espiritual
- As tradiÃ§Ãµes da Umbanda **variam** entre terreiros e regiÃµes
- Sempre cite as fontes e recomende consultar um dirigente para questÃµes especÃ­ficas
- O conteÃºdo ingerido deve ser confiÃ¡vel e autorizado

## ğŸ” Dados Locais

- Nenhum dado Ã© enviado para serviÃ§os externos
- Tudo roda localmente: embeddings, busca, Ã­ndices
- Os PDFs e Ã­ndices ficam em `backend/data/`

## ğŸ“¦ DependÃªncias

### Backend
- FastAPI, Uvicorn
- FAISS (busca vetorial)
- Sentence Transformers (embeddings)
- PyMuPDF (parsing de PDFs)
- Pydantic (validaÃ§Ã£o)

### Frontend
- React, React DOM
- Vite (bundler)
- TypeScript
- Tailwind CSS
- TanStack Query (gerenciamento de estado)

## ğŸ¤ Contribuindo

1. Ingira novos PDFs em `backend/data/pdfs/`
2. Execute `python backend/ingest.py` para atualizar o Ã­ndice
3. Envie feedback e melhore a plataforma

## ğŸ“„ LicenÃ§a

MIT (ou conforme vocÃª preferir)

---

**Status:** MVP local-first, sem Docker, sem serviÃ§os pagos.
